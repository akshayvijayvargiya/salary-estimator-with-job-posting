{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "577337a1",
   "metadata": {},
   "source": [
    "<center>\n",
    "<div style=\"color:white;\n",
    "           display:fill;\n",
    "           border-radius:5px;\n",
    "           background-color: royalblue;\n",
    "           font-size:110%;\n",
    "           letter-spacing:0.5px\">\n",
    "\n",
    "<h4 style=\"padding: 10px;\n",
    "              color:white;\">We will be scrapping data here from job listing websites\n",
    "\n",
    "    \n",
    "</h4>\n",
    "</div>\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69b2669",
   "metadata": {},
   "source": [
    "\n",
    "**Web scraping** is a technique used to extract large amounts of data from websites. It is done by writing computer programs that visit web pages, parse the contents of the page, and extract data for further processing. Web scraping is important in data science because it allows businesses to collect data from websites, including news sites, social media, and other online sources, which can be used to analyze trends, measure customer sentiment, and create predictive models. \n",
    "\n",
    "It is also used to populate databases, monitor prices, and collect other types of information that can be used in data science projects. Web scraping is a powerful tool for data science, as it allows businesses to collect and analyze data from different sources quickly and cost-effectively.\n",
    "\n",
    "**What are the several ways of webscraping?**\n",
    "\n",
    "1. **API**: API (Application Programming Interface) is a set of protocols, routines, and tools for building software applications. APIs can be used to access web-based data and services, such as webpages, images, and other content. \n",
    "\n",
    "2. **HTML Parsers**: HTML parsers are programs that parse HTML documents and extract the data from them. They are usually used to access webpages and extract the content from them.\n",
    "\n",
    "3. **Scrapy**: Scrapy is a Python-based web scraping framework that provides a complete toolkit for scraping websites. It can be used to crawl multiple pages, extract data from them, and store it in a structured format.\n",
    "\n",
    "4. **Selenium**: Selenium is an open-source tool for automating web browsers. It can be used to simulate user actions on a web page and extract data from it.\n",
    "\n",
    "5. **Wget/Curl**: Wget and Curl are command-line programs used to download webpages and other web-based resources. They can be used to access and scrape webpages.\n",
    "\n",
    "We will be choosing the HTML Parser method for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4038e229",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7e6c09",
   "metadata": {},
   "source": [
    "### Making a request to the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bf7e165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The response that we got back from the URL is 200\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.shine.com/job-search/data-scientist-jobs?top_companies_boost=true&q=data-scientist'\n",
    "response = requests.get(url)\n",
    "print('The response that we got back from the URL is', response.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6ca727",
   "metadata": {},
   "source": [
    "#### Since we are getting 200 as response object, we are ready to proceed ahead.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<p align=\"center\">A Response 200 is an HTTP status code that signifies a successful request. This code is returned when a web browser or client successfully receives the requested web page from a server. It indicates that the server has processed the request without errors and has provided the requested content, typically in the form of a web page, document, or data. </p>\n",
    "</div>\n",
    "\n",
    "**BEAUTIFULSOUP**\n",
    "\n",
    "For Scraping we will be using a Python Library called BeautifulSoup. Beautiful Soup is a Python library used for parsing HTML and XML documents. It creates a parse tree for parsed pages that can be used to extract data from HTML, which is useful for web scraping. Beautiful Soup provides a few simple methods and Pythonic idioms for navigating, searching, and modifying a parse tree: a toolkit for dissecting a document and extracting what you need."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb222112",
   "metadata": {},
   "source": [
    "### Parsing HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c98718b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text,'html.parser')\n",
    "html = soup.find_all('div')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057e1104",
   "metadata": {},
   "source": [
    "The HTML contains the contents of the website in the HTML format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d78e0e",
   "metadata": {},
   "source": [
    "### Fetching the Job titles\n",
    "\n",
    "The select method allows a user to query a page using a CSS selector to find elements matching the selector. This can be used to find specific tags, classes, or ids in the page's HTML. Remember to not use .text method on a list as it will result in an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d580bbe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Data Scientist',\n",
       " 'Data Scientists 2024',\n",
       " 'Data Scientist Vacancy',\n",
       " 'Data Scientist Vacancy',\n",
       " 'Data Scientist Vacancy']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "req = soup.select('div h2[itemprop=\"name\"]')\n",
    "#fetching the text from the html\n",
    "titles = [r.text for r in req]\n",
    "#Removing any spaces\n",
    "titles = [t.replace(\"  \", \"\") for t in titles]\n",
    "titles[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edde8fb5",
   "metadata": {},
   "source": [
    "### Fetching the Company Name\n",
    "\n",
    "FIND ALL\n",
    "- The `find_all()` method in `BeautifulSoup` searches through a parsed document and returns a collection of all tags that match the criteria specified in the functionâ€™s arguments. \n",
    "- It searches through the entire tree of parsed code and returns a list of tags that match the given criteria. \n",
    "- It is an efficient way to search for specific tags and their contents in a parsed document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0901cb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "orgs = soup.find_all('div', class_='jobCard_jobCard_cName__mYnow')\n",
    "#fetching the text from the HTML\n",
    "orgs1 = [o.text for o in orgs]\n",
    "sub_string ='Hiring'\n",
    "#Splitting the string on a sub string and getting the first index (Cleaning up names)\n",
    "orgs1 = [o.split(sub_string)[0] for o in orgs1]\n",
    "#Removing any spaces\n",
    "orgs1 = [o.strip() for o in orgs1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff7dca4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Advance Immigrations',\n",
       " 'Bosch Group',\n",
       " 'Divya Interprises',\n",
       " 'Divya Interprises',\n",
       " 'Divya Interprises']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orgs1[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64788bf8",
   "metadata": {},
   "source": [
    "### Fetching the Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "751ded17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetching the HTML data from the class where the location data is available\n",
    "loc = soup.find_all('div', class_='jobCard_jobCard_lists__fdnsc')\n",
    "#fetching all the text from the HTML\n",
    "location = [l.text for l in loc]\n",
    "#cleaning the locations (Getting everything after the Yr(s))\n",
    "location = [re.findall(\"Yrs?(.*)$\", i)[0] for i in location]\n",
    "#Getting rid of unnecessary text\n",
    "location = [l.replace(\"+4\", \", \") for l in location]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e416f781",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Australia+2Canada, Singapore',\n",
       " 'Coimbatore',\n",
       " 'Oman+14Qatar, Asansol, Saudi Arabia, Dibrugarh, Guwahati, Shimla, Kuwait, Dehradun, Indore, Jabalpur, United Arab Emirates, Kolkata, Shillong, Bhopal',\n",
       " 'Oman+14Katni, Qatar, Ratlam, Saudi Arabia, Jammu, Kuwait, Bangalore, Kochi, United Arab Emirates, Vellore, Hosur, Pune, Mangalore, Bhopal',\n",
       " 'Oman+14Qatar, Saudi Arabia, Bhubaneswar, Jammu, Cuttack, Dehradun, Rourkela, Jaipur, Kuwait, Srinagar, Jodhpur, United Arab Emirates, Udaipur, Pune']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "location[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ac3cf6",
   "metadata": {},
   "source": [
    "### Fetching Experience\n",
    "\n",
    "We will use `re` which is a library meant for regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bafe0809",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetching the text from the loc variable for the experience\n",
    "exp = [l.text for l in loc]\n",
    "#Cleaning up using regex\n",
    "experience = [re.findall(\"^(.*) Yrs?\", i)[0] for i in exp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63794ba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1 to 6', '0 to 4', '0 to 4', '0 to 4', '0 to 4']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experience[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4f8d1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Getting Vacancies\n",
    "\n",
    "vac = soup.find_all('ul', class_='jobCard_jobCard_jobDetail__jD82J')\n",
    "#fetching the text from the HTML\n",
    "vac = [v.text for v in vac ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e767b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning up the data\n",
    "vacancies = [int(re.findall(r'\\d+', text)[0]) if re.findall(r'\\d+', text) else 1 for text in vac]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab373a3",
   "metadata": {},
   "source": [
    "<center>\n",
    "<div style=\"color:white;\n",
    "           display:fill;\n",
    "           border-radius:5px;\n",
    "           background-color: royalblue;\n",
    "           font-size:110%;\n",
    "           letter-spacing:0.5px\">\n",
    "\n",
    "<h4 style=\"padding: 10px;\n",
    "              color:white;\"> Putting the pieces together\n",
    "\n",
    "    \n",
    "</h4>\n",
    "</div>\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aebe35ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Titles':titles, 'Firm Name': orgs1, \n",
    "        'Job Location':location, 'Experience':experience,\n",
    "        'Positions': vacancies}\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f3b31c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Titles</th>\n",
       "      <th>Firm Name</th>\n",
       "      <th>Job Location</th>\n",
       "      <th>Experience</th>\n",
       "      <th>Positions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Advance Immigrations</td>\n",
       "      <td>Australia+2Canada, Singapore</td>\n",
       "      <td>1 to 6</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientists 2024</td>\n",
       "      <td>Bosch Group</td>\n",
       "      <td>Coimbatore</td>\n",
       "      <td>0 to 4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist Vacancy</td>\n",
       "      <td>Divya Interprises</td>\n",
       "      <td>Oman+14Qatar, Asansol, Saudi Arabia, Dibrugarh...</td>\n",
       "      <td>0 to 4</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist Vacancy</td>\n",
       "      <td>Divya Interprises</td>\n",
       "      <td>Oman+14Katni, Qatar, Ratlam, Saudi Arabia, Jam...</td>\n",
       "      <td>0 to 4</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist Vacancy</td>\n",
       "      <td>Divya Interprises</td>\n",
       "      <td>Oman+14Qatar, Saudi Arabia, Bhubaneswar, Jammu...</td>\n",
       "      <td>0 to 4</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Titles             Firm Name  \\\n",
       "0          Data Scientist  Advance Immigrations   \n",
       "1    Data Scientists 2024           Bosch Group   \n",
       "2  Data Scientist Vacancy     Divya Interprises   \n",
       "3  Data Scientist Vacancy     Divya Interprises   \n",
       "4  Data Scientist Vacancy     Divya Interprises   \n",
       "\n",
       "                                        Job Location Experience  Positions  \n",
       "0                       Australia+2Canada, Singapore     1 to 6         20  \n",
       "1                                         Coimbatore     0 to 4          1  \n",
       "2  Oman+14Qatar, Asansol, Saudi Arabia, Dibrugarh...     0 to 4         99  \n",
       "3  Oman+14Katni, Qatar, Ratlam, Saudi Arabia, Jam...     0 to 4         99  \n",
       "4  Oman+14Qatar, Saudi Arabia, Bhubaneswar, Jammu...     0 to 4         99  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ae4e94",
   "metadata": {},
   "source": [
    "### The same way, we can fetch more job findings from different websites, like indeed.com.\n",
    "Though it is difficult now to get data out form indeed due to bots, but we managed to get it form there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98721c9c",
   "metadata": {},
   "source": [
    "Now if there are several pages one could also write a loop for the rest of the pages which is what has been done in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "edb92efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have managed to fetch 100 job postings while scraping 5 pages.\n"
     ]
    }
   ],
   "source": [
    "TITLES = []\n",
    "COMPANIES = []\n",
    "LOCATIONS = []\n",
    "EXPERIENCE = []\n",
    "VACANCIES = []\n",
    "\n",
    "Range = range(1,6)\n",
    "for i in Range:\n",
    "    link = f'https://www.shine.com/job-search/data-scientist-jobs-{i}?top_companies_boost=true&sort=1&q=data-scientist'\n",
    "    response = requests.get(link)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        req = soup.select('div h2[itemprop=\"name\"]')\n",
    "        titles = [r.text for r in req]\n",
    "        titles1 = [t.replace(\"|\",\"\") for t in titles]\n",
    "        titles = [t.replace(\"  \", \"\") for t in titles1]\n",
    "        TITLES.extend(titles)\n",
    "        \n",
    "        orgs = soup.find_all('div', class_='jobCard_jobCard_cName__mYnow')\n",
    "        orgs1 = [o.text for o in orgs]\n",
    "        sub_str = \"Hiring\"\n",
    "        companies = [o.split(sub_str)[0] for o in orgs1]\n",
    "        COMPANIES.extend(companies) \n",
    "        \n",
    "        loc = soup.find_all('div', class_='jobCard_jobCard_lists__fdnsc')\n",
    "        location = [l.text for l in loc]\n",
    "        location = [re.findall(\"Yrs?(.*)$\", i)[0] for i in location]\n",
    "        location = [l.replace(\"+4\", \", \") for l in location]\n",
    "        LOCATIONS.extend(location)\n",
    "        \n",
    "        #Fetching Experience\n",
    "        exp = [l.text for l in loc]\n",
    "        experience = [re.findall(\"^(.*) Yrs?\", i)[0] for i in exp]\n",
    "        EXPERIENCE.extend(experience)  \n",
    "        \n",
    "        vacancies = soup.find_all('ul', class_='jobCard_jobCard_jobDetail__jD82J')\n",
    "        vac = [v.text for v in vacancies]\n",
    "        vacancies = [int(re.findall(r'\\d+', text)[0]) if re.findall(r'\\d+', text) else 1 for text in vac]\n",
    "        VACANCIES.extend(vacancies)\n",
    "        \n",
    "    else:\n",
    "        print('Invalid Response')\n",
    "\n",
    "df = pd.DataFrame({'Job Title': TITLES, \n",
    "                   'Employer': COMPANIES,\n",
    "                   'Job Location': LOCATIONS, \n",
    "                   'Experience': EXPERIENCE, \n",
    "                   'Positions': VACANCIES})\n",
    "\n",
    "print(f'We have managed to fetch {len(df)} job postings while scraping {len(Range)} pages.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cdd9baa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping the duplicates and adding the new column\n",
    "df = df.drop_duplicates(subset=['Job Title'])\n",
    "df['Category'] = ['Fresher' if '0' in i else 'Experienced' for i in df['Experience']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "289297da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Employer</th>\n",
       "      <th>Job Location</th>\n",
       "      <th>Experience</th>\n",
       "      <th>Positions</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Power Bi Developer</td>\n",
       "      <td>FUTURE A4</td>\n",
       "      <td>Kochi</td>\n",
       "      <td>2 to 7</td>\n",
       "      <td>5</td>\n",
       "      <td>Experienced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Business Analyst - Trading</td>\n",
       "      <td>REVA HR SOLUTIONS.</td>\n",
       "      <td>Bangalore+1Pune</td>\n",
       "      <td>7 to 10</td>\n",
       "      <td>3</td>\n",
       "      <td>Fresher</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Advance Immigrations</td>\n",
       "      <td>Australia+2Canada, Singapore</td>\n",
       "      <td>1 to 6</td>\n",
       "      <td>20</td>\n",
       "      <td>Experienced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Required Business Analyst in Europe</td>\n",
       "      <td>AMEURO MIGRATION PRIVATE LIMITED</td>\n",
       "      <td>Germany</td>\n",
       "      <td>4 to 9</td>\n",
       "      <td>46</td>\n",
       "      <td>Experienced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Management Trainee India (U.Gro Next Level)</td>\n",
       "      <td>FUGRO SURVEY INDIA PRIVATE LIMITED</td>\n",
       "      <td>Navi Mumbai</td>\n",
       "      <td>0 to 2</td>\n",
       "      <td>2</td>\n",
       "      <td>Fresher</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>sales officer</td>\n",
       "      <td>HAPPY SQUARE OUTSOURCING SERVICES P...</td>\n",
       "      <td>Ahmedabad</td>\n",
       "      <td>1 to 6</td>\n",
       "      <td>1</td>\n",
       "      <td>Experienced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Sales officer</td>\n",
       "      <td>HAPPY SQUARE OUTSOURCING SERVICES P...</td>\n",
       "      <td>Bhubaneswar</td>\n",
       "      <td>1 to 5</td>\n",
       "      <td>1</td>\n",
       "      <td>Experienced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Invoicing and Documentation Specialist</td>\n",
       "      <td>HAPPY SQUARE OUTSOURCING SERVICES P...</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>2 to 5</td>\n",
       "      <td>1</td>\n",
       "      <td>Experienced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>UrgentWipro-Data Engineering</td>\n",
       "      <td>Wipro Technologies</td>\n",
       "      <td>Bangalore+6Noida, Chennai, Hyderabad, Gurugram...</td>\n",
       "      <td>6 to 11</td>\n",
       "      <td>1</td>\n",
       "      <td>Experienced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Quess Corp (Magna Infotech)</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>8 to 13</td>\n",
       "      <td>1</td>\n",
       "      <td>Experienced</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>93 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Job Title  \\\n",
       "0                            Power Bi Developer   \n",
       "1                    Business Analyst - Trading   \n",
       "2                                Data Scientist   \n",
       "3           Required Business Analyst in Europe   \n",
       "4   Management Trainee India (U.Gro Next Level)   \n",
       "..                                          ...   \n",
       "95                                sales officer   \n",
       "96                                Sales officer   \n",
       "97       Invoicing and Documentation Specialist   \n",
       "98                 UrgentWipro-Data Engineering   \n",
       "99                                Data Engineer   \n",
       "\n",
       "                                  Employer  \\\n",
       "0                                FUTURE A4   \n",
       "1                       REVA HR SOLUTIONS.   \n",
       "2                     Advance Immigrations   \n",
       "3         AMEURO MIGRATION PRIVATE LIMITED   \n",
       "4       FUGRO SURVEY INDIA PRIVATE LIMITED   \n",
       "..                                     ...   \n",
       "95  HAPPY SQUARE OUTSOURCING SERVICES P...   \n",
       "96  HAPPY SQUARE OUTSOURCING SERVICES P...   \n",
       "97  HAPPY SQUARE OUTSOURCING SERVICES P...   \n",
       "98                      Wipro Technologies   \n",
       "99             Quess Corp (Magna Infotech)   \n",
       "\n",
       "                                         Job Location Experience  Positions  \\\n",
       "0                                               Kochi     2 to 7          5   \n",
       "1                                     Bangalore+1Pune    7 to 10          3   \n",
       "2                        Australia+2Canada, Singapore     1 to 6         20   \n",
       "3                                             Germany     4 to 9         46   \n",
       "4                                         Navi Mumbai     0 to 2          2   \n",
       "..                                                ...        ...        ...   \n",
       "95                                          Ahmedabad     1 to 6          1   \n",
       "96                                        Bhubaneswar     1 to 5          1   \n",
       "97                                          Bangalore     2 to 5          1   \n",
       "98  Bangalore+6Noida, Chennai, Hyderabad, Gurugram...    6 to 11          1   \n",
       "99                                          Bangalore    8 to 13          1   \n",
       "\n",
       "       Category  \n",
       "0   Experienced  \n",
       "1       Fresher  \n",
       "2   Experienced  \n",
       "3   Experienced  \n",
       "4       Fresher  \n",
       "..          ...  \n",
       "95  Experienced  \n",
       "96  Experienced  \n",
       "97  Experienced  \n",
       "98  Experienced  \n",
       "99  Experienced  \n",
       "\n",
       "[93 rows x 6 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c548ad59",
   "metadata": {},
   "source": [
    "## Scrapping Indeed.com\n",
    "\n",
    "This doesn't needed to be run all the time because the bots on indeed.com doesn't allow us to scrap data most of the time. We managed to grab the data and put that to a csv file in the folder. We can use that to our next set of analysis.\n",
    "Most of the time the indeed.com bots return bogus response and we can't decrypt the data out of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb92701c",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"http://www.indeed.com/jobs?q=data+scientist+%2420%2C000&l=New+York&start=10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd2a2130",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e5aba93",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 500\n",
    "pd.options.display.max_columns = 100\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b2881361",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the request\n",
    "r = requests.get(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1500c88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(r.text,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "484aac9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to scrape the location\n",
    "def extract_location_from_result(result):\n",
    "    location = []\n",
    "    for l in result.find_all('div', attrs={'class':'jobsearch-SerpJobCard'}):\n",
    "        try:\n",
    "            location.append(l.find(('div', 'span'), attrs={'class':'location'}).text)\n",
    "        except:\n",
    "            location.append(np.nan)\n",
    "    return location\n",
    "\n",
    "#Function to scrape the salary\n",
    "def extract_salary_from_result(result):\n",
    "    salary = []\n",
    "    for s in result.find_all('div', attrs={'class':'jobsearch-SerpJobCard'}):\n",
    "        try:\n",
    "            salary.append(s.find(('div', 'span'), attrs={'class':'salary no-wrap'}).text.strip())\n",
    "        except:\n",
    "            salary.append(np.nan)\n",
    "    return salary\n",
    "\n",
    "#Function to scrape the job\n",
    "def extract_job_from_result(result):\n",
    "    job = []\n",
    "    for j in result.find_all('div', attrs={'class':'jobsearch-SerpJobCard'}):\n",
    "        try:\n",
    "            job.append(j.find('a', attrs={'data-tn-element':'jobTitle'}).text.strip())\n",
    "        except:\n",
    "            job.append(np.nan)\n",
    "    return job\n",
    "\n",
    "#Function to scrape the company\n",
    "def extract_company_from_result(result):\n",
    "    company = []\n",
    "    for c in result.find_all('div', attrs={'class':'jobsearch-SerpJobCard'}):\n",
    "        try:\n",
    "            company.append(c.find(('div', 'span'), attrs={'class':'company'}).text.strip())\n",
    "        except:\n",
    "            company.append(np.nan)\n",
    "    return company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "014f34a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "YOUR_CITY = 'Seattle','Montgomery', 'San+Antonio', 'San+Diego', 'San+Jose', 'Jacksonville', 'Columbus', 'Fort+Worth', 'Indianapolis', 'Charlotte', 'Boston', 'Washington+DC', 'El+Paso', 'Detroit', 'Nashville', 'Memphis', 'Portland', 'Oklahoma+City', 'Las+Vegas'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "66bef9f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ddbc0104cd344c8a4df8c712a5e742d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30fd8323ee1841ab8fea75d4db26fe1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_results_per_city = 1000 \n",
    "\n",
    "results_location = []\n",
    "results_job = []\n",
    "results_salary = []\n",
    "results_company = []\n",
    "\n",
    "for city in set(['New+York', 'Chicago']):\n",
    "    temp_location = []\n",
    "    temp_job = []\n",
    "    temp_salary = []\n",
    "    temp_company = []\n",
    "    \n",
    "    for start in tqdm_notebook (range(0, max_results_per_city, 10)):\n",
    "        url = 'http://www.indeed.com/jobs?q=data+scientist+%2420%2C000&l={}&start={}'.format(city,start)\n",
    "        r = requests.get(url)\n",
    "        global_soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        \n",
    "        temp_company.append(extract_company_from_result(global_soup))\n",
    "        temp_job.append(extract_job_from_result(global_soup))\n",
    "        temp_salary.append(extract_salary_from_result(global_soup))\n",
    "        temp_location.append(extract_location_from_result(global_soup))\n",
    "        \n",
    "    results_location.append(temp_location)\n",
    "    results_job.append(temp_job)\n",
    "    results_salary.append(temp_salary)\n",
    "    results_company.append(temp_company)\n",
    "\n",
    "   \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "67161a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'Job': [item for l in results_job for sublist in l for item in sublist], \n",
    "    'Company': [item for l in results_company for sublist in l for item in sublist], \n",
    "    'Salary': [item for l in results_salary for sublist in l for item in sublist], \n",
    "    'Location':[item for l in results_location for sublist in l for item in sublist]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "344a88d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Data_Science.csv', sep='\\t', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d007082d",
   "metadata": {},
   "source": [
    "## CONCLUSION\n",
    "\n",
    "In this Jupyter Notebook, we have implemented a web scraping script to extract job-related data from a website. The script goes through the following steps to gather information:\n",
    "\n",
    "1. **Sending HTTP Requests**: We use the `requests` library to send HTTP GET requests to the constructed URLs. We check the status code of the response to ensure that we receive a successful response (HTTP status code 200) before proceeding.\n",
    "\n",
    "2. **Parsing HTML Content**: Upon successful retrieval of a web page, we used the `BeautifulSoup` library to parse the HTML content of the page. This enabled us to navigate the page's structure and extract relevant information.\n",
    "\n",
    "3. **Extracting Data**: We learnt how to navigate the HTML of a website to get the required data, stored in various classes, we are after. In our case, we went to get the job titles, company names, locations, and experience requirements. We also created a new column based on our existing data.\n",
    "\n",
    "4. **Looping through Pages**: Once we got the hang of how to do this for one page, we iterated through a range of pages, where each page contains a list of job postings. For each page, we construct a URL with the page number and specific query parameters related to job type, top companies, and sorting.\n",
    "\n",
    "5. **Visualizing Data**: We also took a look at some charts to understand our data i.e what is data distribution of Number of Positions, How many jobs in the market are for freshers and for experienced professionals.\n",
    "\n",
    "By following these steps, we systematically gather data from multiple web pages, ensuring that we handle variations in the format of information such as job titles, company names, locations, and experience requirements. The use of regular expressions allowed us to extract specific patterns from the text, enhancing the accuracy of our data extraction process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
